{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled337.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOiC5k9IrEiG330E+DWm+k8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyasi345/nlp_chatbot/blob/master/tf_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIUIa44_GcPH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "20b687d0-afe0-4d11-d5f4-2b9e1aff54ed"
      },
      "source": [
        "!unzip '/content/chatbot.zip'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/chatbot.zip\n",
            "   creating: chatbot/\n",
            "  inflating: chatbot/datasets_110245_263384_ai.yml  \n",
            "  inflating: chatbot/datasets_110245_263384_emotion.yml  \n",
            "  inflating: chatbot/datasets_110245_263384_humor.yml  \n",
            "  inflating: chatbot/datasets_110245_263384_psychology.yml  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kyNh0FQD07V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb01153d-405f-4743-9fda-740d16b5e77e"
      },
      "source": [
        "\n",
        "from tensorflow.keras import preprocessing , utils\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "dir_path = '/content/chatbot'\n",
        "files_list = os.listdir(dir_path + os.sep)\n",
        "\n",
        "questions = list()\n",
        "answers = list()\n",
        "\n",
        "for filepath in files_list:\n",
        "    stream = open( dir_path + os.sep + filepath , 'rb')\n",
        "    docs = yaml.safe_load(stream)\n",
        "    conversations = docs['conversations']\n",
        "    for con in conversations:\n",
        "        if len( con ) > 2 :\n",
        "            questions.append(con[0])\n",
        "            replies = con[ 1 : ]\n",
        "            ans = ''\n",
        "            for rep in replies:\n",
        "                ans += ' ' + rep\n",
        "            answers.append( ans )\n",
        "        elif len( con )> 1:\n",
        "            questions.append(con[0])\n",
        "            answers.append(con[1])\n",
        "\n",
        "answers_with_tags = list()\n",
        "for i in range( len( answers ) ):\n",
        "    if type( answers[i] ) == str:\n",
        "        answers_with_tags.append( answers[i] )\n",
        "    else:\n",
        "        questions.pop( i )\n",
        "\n",
        "answers = list()\n",
        "for i in range( len( answers_with_tags ) ) :\n",
        "    answers.append( '<START> ' + answers_with_tags[i] + ' <END>' )\n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( questions + answers )\n",
        "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
        "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VOCAB SIZE : 1011\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1TVi9J0Hy8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras import layers , activations , models , preprocessing\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-loa1u9IHp_I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "f1b7af50-e80b-41aa-a22f-aa778bab5fc2"
      },
      "source": [
        "\n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "\n",
        "vocab = []\n",
        "for word in tokenizer.word_index:\n",
        "    vocab.append( word )\n",
        "\n",
        "def tokenize( sentences ):\n",
        "    tokens_list = []\n",
        "    vocabulary = []\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower()\n",
        "        sentence = re.sub( '[^a-zA-Z]', ' ', sentence )\n",
        "        tokens = sentence.split()\n",
        "        vocabulary += tokens\n",
        "        tokens_list.append( tokens )\n",
        "    return tokens_list , vocabulary\n",
        "\n",
        "#p = tokenize( questions + answers )\n",
        "#model = Word2Vec( p[ 0 ] ) \n",
        "\n",
        "#embedding_matrix = np.zeros( ( VOCAB_SIZE , 100 ) )\n",
        "#for i in range( len( tokenizer.word_index ) ):\n",
        "    #embedding_matrix[ i ] = model[ vocab[i] ]\n",
        "\n",
        "# encoder_input_data\n",
        "tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
        "maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
        "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\n",
        "encoder_input_data = np.array( padded_questions )\n",
        "print( encoder_input_data.shape , maxlen_questions )\n",
        "\n",
        "# decoder_input_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
        "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
        "decoder_input_data = np.array( padded_answers )\n",
        "print( decoder_input_data.shape , maxlen_answers )\n",
        "\n",
        "# decoder_output_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
        "for i in range(len(tokenized_answers)) :\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
        "onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
        "decoder_output_data = np.array( onehot_answers )\n",
        "print( decoder_output_data.shape )\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(295, 9) 9\n",
            "(295, 68) 68\n",
            "(295, 68, 1011)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3te-YtAH0-h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "04c887fa-f2f8-4354-9b03-cc2b80ae798e"
      },
      "source": [
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape=( maxlen_questions , ))\n",
        "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
        "encoder_states = [ state_h , state_c ]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=( maxlen_answers ,  ))\n",
        "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
        "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
        "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
        "output = decoder_dense ( decoder_outputs )\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 9)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, 68)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 9, 200)       202200      input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 68, 200)      202200      input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 200), (None, 320800      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, 68, 200), (N 320800      embedding_3[0][0]                \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 68, 1011)     203211      lstm_3[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,249,211\n",
            "Trainable params: 1,249,211\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVwPPjF2H6qT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5a89a08a-a53f-40ce-c83d-90c02a6cd432"
      },
      "source": [
        "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=50, epochs=150 ) \n",
        "model.save( 'model.h5' ) \n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "6/6 [==============================] - 3s 437ms/step - loss: 1.5151\n",
            "Epoch 2/150\n",
            "6/6 [==============================] - 3s 430ms/step - loss: 1.2652\n",
            "Epoch 3/150\n",
            "6/6 [==============================] - 3s 429ms/step - loss: 1.2281\n",
            "Epoch 4/150\n",
            "6/6 [==============================] - 3s 422ms/step - loss: 1.2123\n",
            "Epoch 5/150\n",
            "6/6 [==============================] - 3s 431ms/step - loss: 1.2010\n",
            "Epoch 6/150\n",
            "6/6 [==============================] - 3s 425ms/step - loss: 1.1875\n",
            "Epoch 7/150\n",
            "6/6 [==============================] - 3s 425ms/step - loss: 1.1756\n",
            "Epoch 8/150\n",
            "6/6 [==============================] - 3s 424ms/step - loss: 1.1614\n",
            "Epoch 9/150\n",
            "6/6 [==============================] - 3s 428ms/step - loss: 1.1478\n",
            "Epoch 10/150\n",
            "6/6 [==============================] - 3s 422ms/step - loss: 1.1351\n",
            "Epoch 11/150\n",
            "6/6 [==============================] - 3s 425ms/step - loss: 1.1222\n",
            "Epoch 12/150\n",
            "6/6 [==============================] - 3s 423ms/step - loss: 1.1100\n",
            "Epoch 13/150\n",
            "6/6 [==============================] - 3s 429ms/step - loss: 1.0978\n",
            "Epoch 14/150\n",
            "6/6 [==============================] - 3s 423ms/step - loss: 1.0833\n",
            "Epoch 15/150\n",
            "6/6 [==============================] - 3s 428ms/step - loss: 1.0705\n",
            "Epoch 16/150\n",
            "6/6 [==============================] - 3s 425ms/step - loss: 1.0558\n",
            "Epoch 17/150\n",
            "6/6 [==============================] - 3s 427ms/step - loss: 1.0426\n",
            "Epoch 18/150\n",
            "6/6 [==============================] - 3s 440ms/step - loss: 1.0279\n",
            "Epoch 19/150\n",
            "6/6 [==============================] - 3s 439ms/step - loss: 1.0143\n",
            "Epoch 20/150\n",
            "6/6 [==============================] - 3s 434ms/step - loss: 1.0010\n",
            "Epoch 21/150\n",
            "6/6 [==============================] - 3s 438ms/step - loss: 0.9863\n",
            "Epoch 22/150\n",
            "6/6 [==============================] - 3s 442ms/step - loss: 0.9722\n",
            "Epoch 23/150\n",
            "6/6 [==============================] - 3s 434ms/step - loss: 0.9613\n",
            "Epoch 24/150\n",
            "6/6 [==============================] - 3s 427ms/step - loss: 0.9454\n",
            "Epoch 25/150\n",
            "6/6 [==============================] - 3s 434ms/step - loss: 0.9338\n",
            "Epoch 26/150\n",
            "6/6 [==============================] - 3s 443ms/step - loss: 0.9186\n",
            "Epoch 27/150\n",
            "6/6 [==============================] - 3s 427ms/step - loss: 0.9066\n",
            "Epoch 28/150\n",
            "6/6 [==============================] - 3s 430ms/step - loss: 0.8943\n",
            "Epoch 29/150\n",
            "6/6 [==============================] - 3s 430ms/step - loss: 0.8818\n",
            "Epoch 30/150\n",
            "6/6 [==============================] - 3s 431ms/step - loss: 0.8697\n",
            "Epoch 31/150\n",
            "6/6 [==============================] - 3s 422ms/step - loss: 0.8599\n",
            "Epoch 32/150\n",
            "6/6 [==============================] - 3s 434ms/step - loss: 0.8456\n",
            "Epoch 33/150\n",
            "6/6 [==============================] - 3s 435ms/step - loss: 0.8329\n",
            "Epoch 34/150\n",
            "6/6 [==============================] - 3s 425ms/step - loss: 0.8241\n",
            "Epoch 35/150\n",
            "6/6 [==============================] - 3s 447ms/step - loss: 0.8103\n",
            "Epoch 36/150\n",
            "6/6 [==============================] - 3s 431ms/step - loss: 0.8005\n",
            "Epoch 37/150\n",
            "6/6 [==============================] - 3s 429ms/step - loss: 0.7900\n",
            "Epoch 38/150\n",
            "6/6 [==============================] - 3s 435ms/step - loss: 0.7772\n",
            "Epoch 39/150\n",
            "6/6 [==============================] - 3s 433ms/step - loss: 0.7668\n",
            "Epoch 40/150\n",
            "6/6 [==============================] - 3s 432ms/step - loss: 0.7559\n",
            "Epoch 41/150\n",
            "6/6 [==============================] - 3s 430ms/step - loss: 0.7469\n",
            "Epoch 42/150\n",
            "6/6 [==============================] - 3s 436ms/step - loss: 0.7339\n",
            "Epoch 43/150\n",
            "6/6 [==============================] - 3s 428ms/step - loss: 0.7244\n",
            "Epoch 44/150\n",
            "6/6 [==============================] - 3s 433ms/step - loss: 0.7136\n",
            "Epoch 45/150\n",
            "6/6 [==============================] - 3s 428ms/step - loss: 0.7037\n",
            "Epoch 46/150\n",
            "6/6 [==============================] - 3s 428ms/step - loss: 0.6914\n",
            "Epoch 47/150\n",
            "6/6 [==============================] - 3s 429ms/step - loss: 0.6847\n",
            "Epoch 48/150\n",
            "6/6 [==============================] - 3s 428ms/step - loss: 0.6706\n",
            "Epoch 49/150\n",
            "6/6 [==============================] - 3s 433ms/step - loss: 0.6636\n",
            "Epoch 50/150\n",
            "6/6 [==============================] - 3s 429ms/step - loss: 0.6531\n",
            "Epoch 51/150\n",
            "6/6 [==============================] - 3s 434ms/step - loss: 0.6433\n",
            "Epoch 52/150\n",
            "6/6 [==============================] - 3s 431ms/step - loss: 0.6304\n",
            "Epoch 53/150\n",
            "6/6 [==============================] - 3s 427ms/step - loss: 0.6241\n",
            "Epoch 54/150\n",
            "6/6 [==============================] - 3s 439ms/step - loss: 0.6146\n",
            "Epoch 55/150\n",
            "6/6 [==============================] - 3s 427ms/step - loss: 0.6069\n",
            "Epoch 56/150\n",
            "6/6 [==============================] - 3s 430ms/step - loss: 0.5923\n",
            "Epoch 57/150\n",
            "6/6 [==============================] - 3s 430ms/step - loss: 0.5860\n",
            "Epoch 58/150\n",
            "6/6 [==============================] - 3s 431ms/step - loss: 0.5761\n",
            "Epoch 59/150\n",
            "6/6 [==============================] - 3s 443ms/step - loss: 0.5646\n",
            "Epoch 60/150\n",
            "6/6 [==============================] - 3s 437ms/step - loss: 0.5568\n",
            "Epoch 61/150\n",
            "6/6 [==============================] - 3s 431ms/step - loss: 0.5513\n",
            "Epoch 62/150\n",
            "6/6 [==============================] - 3s 427ms/step - loss: 0.5386\n",
            "Epoch 63/150\n",
            "6/6 [==============================] - 3s 436ms/step - loss: 0.5293\n",
            "Epoch 64/150\n",
            "6/6 [==============================] - 3s 427ms/step - loss: 0.5224\n",
            "Epoch 65/150\n",
            "6/6 [==============================] - 3s 421ms/step - loss: 0.5107\n",
            "Epoch 66/150\n",
            "6/6 [==============================] - 3s 435ms/step - loss: 0.5056\n",
            "Epoch 67/150\n",
            "6/6 [==============================] - 3s 441ms/step - loss: 0.4937\n",
            "Epoch 68/150\n",
            "6/6 [==============================] - 3s 434ms/step - loss: 0.4887\n",
            "Epoch 69/150\n",
            "6/6 [==============================] - 3s 425ms/step - loss: 0.4781\n",
            "Epoch 70/150\n",
            "6/6 [==============================] - 3s 423ms/step - loss: 0.4711\n",
            "Epoch 71/150\n",
            "6/6 [==============================] - 3s 424ms/step - loss: 0.4619\n",
            "Epoch 72/150\n",
            "6/6 [==============================] - 3s 424ms/step - loss: 0.4504\n",
            "Epoch 73/150\n",
            "6/6 [==============================] - 3s 431ms/step - loss: 0.4476\n",
            "Epoch 74/150\n",
            "6/6 [==============================] - 3s 429ms/step - loss: 0.4351\n",
            "Epoch 75/150\n",
            "6/6 [==============================] - 3s 431ms/step - loss: 0.4283\n",
            "Epoch 76/150\n",
            "6/6 [==============================] - 3s 430ms/step - loss: 0.4207\n",
            "Epoch 77/150\n",
            "6/6 [==============================] - 3s 432ms/step - loss: 0.4145\n",
            "Epoch 78/150\n",
            "6/6 [==============================] - 3s 434ms/step - loss: 0.4027\n",
            "Epoch 79/150\n",
            "6/6 [==============================] - 3s 425ms/step - loss: 0.4002\n",
            "Epoch 80/150\n",
            "6/6 [==============================] - 3s 435ms/step - loss: 0.3891\n",
            "Epoch 81/150\n",
            "6/6 [==============================] - 3s 433ms/step - loss: 0.3835\n",
            "Epoch 82/150\n",
            "6/6 [==============================] - 3s 434ms/step - loss: 0.3730\n",
            "Epoch 83/150\n",
            "6/6 [==============================] - 3s 431ms/step - loss: 0.3672\n",
            "Epoch 84/150\n",
            "6/6 [==============================] - 3s 424ms/step - loss: 0.3633\n",
            "Epoch 85/150\n",
            "6/6 [==============================] - 3s 434ms/step - loss: 0.3534\n",
            "Epoch 86/150\n",
            "6/6 [==============================] - 3s 438ms/step - loss: 0.3443\n",
            "Epoch 87/150\n",
            "6/6 [==============================] - 3s 442ms/step - loss: 0.3398\n",
            "Epoch 88/150\n",
            "6/6 [==============================] - 3s 434ms/step - loss: 0.3373\n",
            "Epoch 89/150\n",
            "6/6 [==============================] - 3s 436ms/step - loss: 0.3239\n",
            "Epoch 90/150\n",
            "6/6 [==============================] - 3s 436ms/step - loss: 0.3168\n",
            "Epoch 91/150\n",
            "6/6 [==============================] - 3s 442ms/step - loss: 0.3133\n",
            "Epoch 92/150\n",
            "6/6 [==============================] - 3s 427ms/step - loss: 0.3064\n",
            "Epoch 93/150\n",
            "6/6 [==============================] - 3s 434ms/step - loss: 0.2980\n",
            "Epoch 94/150\n",
            "6/6 [==============================] - 3s 425ms/step - loss: 0.2940\n",
            "Epoch 95/150\n",
            "6/6 [==============================] - 3s 426ms/step - loss: 0.2902\n",
            "Epoch 96/150\n",
            "6/6 [==============================] - 3s 427ms/step - loss: 0.2790\n",
            "Epoch 97/150\n",
            "6/6 [==============================] - 3s 429ms/step - loss: 0.2730\n",
            "Epoch 98/150\n",
            "6/6 [==============================] - 3s 438ms/step - loss: 0.2675\n",
            "Epoch 99/150\n",
            "6/6 [==============================] - 3s 449ms/step - loss: 0.2653\n",
            "Epoch 100/150\n",
            "6/6 [==============================] - 3s 437ms/step - loss: 0.2572\n",
            "Epoch 101/150\n",
            "6/6 [==============================] - 3s 438ms/step - loss: 0.2536\n",
            "Epoch 102/150\n",
            "6/6 [==============================] - 3s 424ms/step - loss: 0.2466\n",
            "Epoch 103/150\n",
            "6/6 [==============================] - 3s 430ms/step - loss: 0.2409\n",
            "Epoch 104/150\n",
            "6/6 [==============================] - 3s 430ms/step - loss: 0.2346\n",
            "Epoch 105/150\n",
            "6/6 [==============================] - 3s 428ms/step - loss: 0.2321\n",
            "Epoch 106/150\n",
            "6/6 [==============================] - 3s 425ms/step - loss: 0.2245\n",
            "Epoch 107/150\n",
            "6/6 [==============================] - 3s 437ms/step - loss: 0.2181\n",
            "Epoch 108/150\n",
            "6/6 [==============================] - 3s 430ms/step - loss: 0.2158\n",
            "Epoch 109/150\n",
            "6/6 [==============================] - 3s 434ms/step - loss: 0.2099\n",
            "Epoch 110/150\n",
            "6/6 [==============================] - 3s 436ms/step - loss: 0.2039\n",
            "Epoch 111/150\n",
            "6/6 [==============================] - 3s 424ms/step - loss: 0.2005\n",
            "Epoch 112/150\n",
            "6/6 [==============================] - 3s 441ms/step - loss: 0.1979\n",
            "Epoch 113/150\n",
            "6/6 [==============================] - 3s 437ms/step - loss: 0.1899\n",
            "Epoch 114/150\n",
            "6/6 [==============================] - 3s 432ms/step - loss: 0.1858\n",
            "Epoch 115/150\n",
            "6/6 [==============================] - 3s 427ms/step - loss: 0.1816\n",
            "Epoch 116/150\n",
            "6/6 [==============================] - 3s 426ms/step - loss: 0.1767\n",
            "Epoch 117/150\n",
            "6/6 [==============================] - 3s 427ms/step - loss: 0.1726\n",
            "Epoch 118/150\n",
            "6/6 [==============================] - 3s 429ms/step - loss: 0.1679\n",
            "Epoch 119/150\n",
            "6/6 [==============================] - 3s 433ms/step - loss: 0.1714\n",
            "Epoch 120/150\n",
            "6/6 [==============================] - 3s 440ms/step - loss: 0.1620\n",
            "Epoch 121/150\n",
            "6/6 [==============================] - 3s 435ms/step - loss: 0.1553\n",
            "Epoch 122/150\n",
            "6/6 [==============================] - 3s 435ms/step - loss: 0.1541\n",
            "Epoch 123/150\n",
            "6/6 [==============================] - 3s 428ms/step - loss: 0.1489\n",
            "Epoch 124/150\n",
            "6/6 [==============================] - 3s 425ms/step - loss: 0.1457\n",
            "Epoch 125/150\n",
            "6/6 [==============================] - 3s 436ms/step - loss: 0.1447\n",
            "Epoch 126/150\n",
            "6/6 [==============================] - 3s 432ms/step - loss: 0.1410\n",
            "Epoch 127/150\n",
            "6/6 [==============================] - 3s 423ms/step - loss: 0.1339\n",
            "Epoch 128/150\n",
            "6/6 [==============================] - 3s 434ms/step - loss: 0.1325\n",
            "Epoch 129/150\n",
            "6/6 [==============================] - 3s 422ms/step - loss: 0.1300\n",
            "Epoch 130/150\n",
            "6/6 [==============================] - 3s 427ms/step - loss: 0.1258\n",
            "Epoch 131/150\n",
            "6/6 [==============================] - 3s 426ms/step - loss: 0.1261\n",
            "Epoch 132/150\n",
            "6/6 [==============================] - 3s 429ms/step - loss: 0.1187\n",
            "Epoch 133/150\n",
            "6/6 [==============================] - 3s 424ms/step - loss: 0.1161\n",
            "Epoch 134/150\n",
            "6/6 [==============================] - 3s 423ms/step - loss: 0.1158\n",
            "Epoch 135/150\n",
            "6/6 [==============================] - 3s 425ms/step - loss: 0.1093\n",
            "Epoch 136/150\n",
            "6/6 [==============================] - 3s 421ms/step - loss: 0.1070\n",
            "Epoch 137/150\n",
            "6/6 [==============================] - 3s 423ms/step - loss: 0.1066\n",
            "Epoch 138/150\n",
            "6/6 [==============================] - 3s 433ms/step - loss: 0.1020\n",
            "Epoch 139/150\n",
            "6/6 [==============================] - 3s 434ms/step - loss: 0.1006\n",
            "Epoch 140/150\n",
            "6/6 [==============================] - 3s 425ms/step - loss: 0.0985\n",
            "Epoch 141/150\n",
            "6/6 [==============================] - 3s 425ms/step - loss: 0.0937\n",
            "Epoch 142/150\n",
            "6/6 [==============================] - 3s 425ms/step - loss: 0.0920\n",
            "Epoch 143/150\n",
            "6/6 [==============================] - 3s 431ms/step - loss: 0.0922\n",
            "Epoch 144/150\n",
            "6/6 [==============================] - 3s 438ms/step - loss: 0.0876\n",
            "Epoch 145/150\n",
            "6/6 [==============================] - 3s 421ms/step - loss: 0.0856\n",
            "Epoch 146/150\n",
            "6/6 [==============================] - 3s 436ms/step - loss: 0.0817\n",
            "Epoch 147/150\n",
            "6/6 [==============================] - 3s 424ms/step - loss: 0.0809\n",
            "Epoch 148/150\n",
            "6/6 [==============================] - 3s 427ms/step - loss: 0.0813\n",
            "Epoch 149/150\n",
            "6/6 [==============================] - 3s 443ms/step - loss: 0.0784\n",
            "Epoch 150/150\n",
            "6/6 [==============================] - 3s 429ms/step - loss: 0.0738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vADFb14FIAVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def make_inference_models():\n",
        "    \n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "    \n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    \n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_embedding , initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "    \n",
        "    return encoder_model , decoder_model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXpjDj_2IDhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def str_to_tokens( sentence : str ):\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for word in words:\n",
        "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
        "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCvA8kM-LQDK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "58476445-7adb-4675-9698-428f978e12b6"
      },
      "source": [
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "for _ in range(4):\n",
        "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "        sampled_word = None\n",
        "        for word , index in tokenizer.word_index.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += ' {}'.format( word )\n",
        "                sampled_word = word\n",
        "        \n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
        "            stop_condition = True\n",
        "            \n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "        states_values = [ h , c ] \n",
        "\n",
        "    print( decoded_translation )\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter question : Tell me a joke\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 68) for input Tensor(\"input_5:0\", shape=(None, 68), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            " what do you get when you cross a cat and a lemon end\n",
            "Enter question : you are busy\n",
            " that too end\n",
            "Enter question : you are an ass kisser\n",
            " i always say if you see an ass go by kiss it end\n",
            "Enter question : you are an addict\n",
            " that's certainly true when i like something i always overdo it end\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8zkuLW8KwnR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "outputId": "8802d293-a8f4-4224-c6a0-cdc1369e10d2"
      },
      "source": [
        "!pip install tf-nightly"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/30/11f2f009914d1e1165c27ab95c4d0b90172bcde086bb7eb4b7e4ee981ef9/tf_nightly-2.5.0.dev20200628-cp36-cp36m-manylinux2010_x86_64.whl (322.6MB)\n",
            "\u001b[K     |████████████████████████████████| 322.6MB 49kB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.3.3)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.2)\n",
            "Collecting tf-estimator-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/c4/60f38d4674884517b12216ec5283b5a8e914140103da1c9e2494ce515650/tf_estimator_nightly-2.3.0.dev2020062601-py2.py3-none-any.whl (459kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 41.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.34.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.2.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.4.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n",
            "Collecting tb-nightly<2.4.0a0,>=2.3.0a0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/d8/5be73d8e526a3aa5edfebe38df017a552df4b0727c224c10d15521e0b131/tb_nightly-2.3.0a20200628-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 43.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.30.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.10.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.10.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.18.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (47.3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.17.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.6.0.post3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (4.1.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.6.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.1.0)\n",
            "Installing collected packages: tf-estimator-nightly, tb-nightly, tf-nightly\n",
            "Successfully installed tb-nightly-2.3.0a20200628 tf-estimator-nightly-2.3.0.dev2020062601 tf-nightly-2.5.0.dev20200628\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyKucaqEIKIE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "794fa37c-b485-4532-bf1c-d9a52daa757f"
      },
      "source": [
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model( enc_model )\n",
        "buffer = converter.convert()\n",
        "open( 'enc_model.tflite' , 'wb' ).write( buffer )\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model( dec_model )\n",
        "open( 'dec_model.tflite' , 'wb' ).write( buffer )\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2756024"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xChzLVQG8Tt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}